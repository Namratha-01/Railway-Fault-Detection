# -*- coding: utf-8 -*-
"""PD3_Railway_Track_Fault_Detection(RFD).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xeG80j_auCz7JXmUrULjTEEuoJKEcINL

##Railway Track Fault Detection
#### Team Members: Arvind Boominathan, Supprethaa Shankar, Namratha Jagadeesh

Importing the required libraries
"""

from google.colab import drive
import os
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from PIL import Image
import numpy as np
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras import layers, models, optimizers
from tensorflow.keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense

"""Mounting the Google Drive to import the dataset"""

drive.mount('/content/drive')''

"""Setting the paths for training, test and validation"""

train_dir = '/content/drive/My Drive/Railway Track fault Detection Updated/Train'
val_dir   = '/content/drive/My Drive/Railway Track fault Detection Updated/Validation'
test_dir  = '/content/drive/My Drive/Railway Track fault Detection Updated/Test'

train_defective_fnames = os.listdir(train_dir+'/Defective' )
train_nondefective_fnames = os.listdir(train_dir+'/Non defective')

"""Viewing the structure of the dataset (viewing images)"""

# Parameters for our graph; we'll output images in a 10x10 configuration
nrows = 4
ncols = 4

# Index for iterating over images
pic_index = 0

# Set up matplotlib fig, and size it to fit 4x4 pics
fig = plt.gcf()
fig.set_size_inches(ncols * 4, nrows * 4)

pic_index += 8
next_defective_pix = [os.path.join(train_dir+'/Defective', fname)
                for fname in train_defective_fnames[pic_index-8:pic_index]]
next_nondefective_pix = [os.path.join(train_dir+'/Non defective', fname)
                for fname in train_nondefective_fnames[pic_index-8:pic_index]]


for i, img_path in enumerate(next_defective_pix+next_nondefective_pix):
  # Set up subplot; subplot indices start at 1
  sp = plt.subplot(nrows, ncols, i + 1)
  sp.axis('Off') # Don't show axes (or gridlines)

  img = mpimg.imread(img_path)
  plt.imshow(img)

plt.show()

"""Importing images from the folder to resize it and perform PCA on the images - Feature Engineering"""

def load_images_from_folder(folder, target_size=(200, 200)):
    images = []
    labels = []
    for subfolder in os.listdir(folder):
        subfolder_path = os.path.join(folder, subfolder)
        if os.path.isdir(subfolder_path):
            for filename in os.listdir(subfolder_path):
                img_path = os.path.join(subfolder_path, filename)

                try:
                    # Open image using PIL and resize
                    img = Image.open(img_path).convert("L")  # Convert to grayscale
                    img_resized = img.resize(target_size)
                    img_array = np.array(img_resized)

                    # Flatten and normalize each image
                    img_flat = img_array.flatten().astype(float) / 255.0
                    images.append(img_flat)
                    labels.append(subfolder)
                except Exception as e:
                    print(f"Error processing {img_path}: {e}")

    return np.array(images), np.array(labels)

"""Performing PCA on the images (Dimensionality Reduction)

1. Loading images
"""

X, y = load_images_from_folder(train_dir)

"""  2. Setting 50 components for reduction"""

# Apply PCA
n_components = 50  # Adjust this based on your analysis

"""  3. Creating the PCA for 50 components"""

pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X)

"""  4. Splitting the dataset into training and testing sets as 80% training and 20% validation"""

X_train, X_val, y_train, y_val = train_test_split(X_pca, y, test_size=0.2, random_state=42)

"""  5. Encoding the labels"""

label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)

"""  6. Plotting the explained variance on the number of components"""

explained_variance = pca.explained_variance_ratio_
cumulative_explained_variance = np.cumsum(explained_variance)

plt.plot(range(1, n_components + 1), cumulative_explained_variance, marker='o', linestyle='--')
plt.title('Cumulative Explained Variance')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.show()

"""**Implementation of selected classification algorithm**(Convolutional Neural Networks)

>1. Building the CNN model and training
A simple neural network architecture for binary classification (Defective/Non-Defective) with one hidden layer having 128 neurons and a ReLU activation function, and an output layer with 1 neuron and a sigmoid activation function.
Feature Extraction is implicitly performed by the CNN model (detection of edges, corners etc)
"""

model = Sequential()
model.add(Dense(128, input_dim=n_components, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

""">2. Compiling the model"""

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

""">3. Training the CNN model"""

# Train the model
history=model.fit(X_train, y_train_encoded, epochs=10, batch_size=32, validation_data=(X_val, y_val_encoded))

""">4. Evaluating the model on the validation set"""

# Evaluate the model on the validation set
loss, accuracy = model.evaluate(X_val, y_val_encoded)
print(f'Validation Set Accuracy: {accuracy}')

# Visualize the training and validation accuracy and loss
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
train_loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(train_acc) + 1)

""">4. Plotting the Training and Validation accuracy"""

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs, train_acc, 'bo-', label='Training Accuracy')
plt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.subplot(1, 2, 2)
plt.plot(epochs, train_loss, 'bo-', label='Training Loss')
plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.tight_layout()
plt.show()